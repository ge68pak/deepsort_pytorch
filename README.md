# Deep SORT

## Introduction

This repository contains code for *Simple Online and Realtime Tracking with a Deep Association Metric* (Deep SORT).
We extend the original [SORT](https://github.com/abewley/sort) algorithm to
integrate appearance information based on a deep appearance descriptor.
See the [arXiv preprint](https://arxiv.org/abs/1703.07402) for more information.

## Dependencies

The code is compatible with Python 2.7 and 3. The following dependencies are
needed to run the tracker:

* NumPy
* sklearn
* OpenCV

Additionally, feature generation requires TensorFlow (>= 1.0).

## Installation

First, clone the repository:
```
git clone https://github.com/nwojke/deep_sort.git
```
Then, download pre-generated detections and the CNN checkpoint file from
[here](https://drive.google.com/open?id=18fKzfqnqhqW3s9zwsCbnVJ5XF2JFeqMp).

*NOTE:* The candidate object locations of our pre-generated detections are
taken from the following paper:
```
F. Yu, W. Li, Q. Li, Y. Liu, X. Shi, J. Yan. POI: Multiple Object Tracking with
High Performance Detection and Appearance Feature. In BMTT, SenseTime Group
Limited, 2016.
```
We have replaced the appearance descriptor with a custom deep convolutional
neural network (see below).

## Running the tracker

The following example starts the tracker on one of the
[MOT16 benchmark](https://motchallenge.net/data/MOT16/)
sequences.下载MOT16数据集
We assume resources have been extracted to the repository root directory and
the MOT16 benchmark data is in `./MOT16`:
```
[1 3 6 7 8 12 14]
运行一个跟踪的实例
python3 deep_sort_app.py \
    --sequence_dir=./MOT16/test/MOT16-01 \
    --detection_file=./resources/detections/MOT16_POI_test/MOT16-01.npy \
    --min_confidence=0.3 \
    --nn_budget=100 \
    --display=True

python3 deep_sort_app.py \
    --sequence_dir=./MOT16/test/MOT16-01 \
    --detection_file=./resources/detections/MOT16_test_mars/MOT16-01.npy \
    --min_confidence=0.3 \
    --nn_budget=100 \
    --display=True

python3 deep_sort_app.py \
    --sequence_dir=./MOT16/test/MOT16-03 \
    --detection_file=./resources/detections/MOT16_POI_test/MOT16-03.npy \
    --min_confidence=0.3 \
    --nn_budget=100 \
    --display=True

python3 deep_sort_app.py \
    --sequence_dir=./MOT16/test/MOT16-03 \
    --detection_file=./resources/detections/MOT16_test_mars/MOT16-03.npy \
    --min_confidence=0.3 \
    --nn_budget=100 \
    --display=True
```
Check `python3 deep_sort_app.py -h` for an overview of available options.
There are also scripts in the repository to visualize results, generate videos,
and evaluate the MOT challenge benchmark.
可选参数解释：
--sequence_dir：视频切成图片序列的文件夹，包含帧序列，每一帧的帧号、目标的bbox、
--detection_file：检测的权重文件 .npy
--output_file：输出类似于gt.txt的文件格式，轨迹不再是默认-1，发生变化
--min_confidence：检测结果阈值。低于这个阈值的检测结果将会被忽略
--nms_max_overlap：非极大抑制的阈值
--max_cosine_distance：余弦距离的控制阈值
--nn_budget：描述的区域的最大值
--display：显示目标追踪结果

## Generating detections
生成检测框
生成用于人员重新识别的功能，适用于使用余弦相似度比较行人边界框的视觉外观
Beside the main tracking application, this repository contains a script to
generate features for person re-identification, suitable to compare the visual
appearance of pedestrian bounding boxes using cosine similarity.
The following example generates these features from standard MOT challenge
detections. Again, we assume resources have been extracted to the repository
root directory and MOT16 data is in `./MOT16`:
```
python3 tools/generate_detections.py \
    --model=resources/networks/mars.pb \
    --mot_dir=./MOT16/train \
    --output_dir=./resources/detections/MOT16_mars_train
```
--model：是模型框架的权重参数文件
The model has been generated with TensorFlow 1.5. If you run into
incompatibility, re-export the frozen inference graph to obtain a new
`mars-small128.pb` that is compatible with your version:
```
python tools/freeze_model.py
```
The ``generate_detections.py`` stores for each sequence of the MOT16 dataset
a separate binary file in NumPy native format. Each file contains an array of
shape `Nx138`, where N is the number of detections in the corresponding MOT
sequence. The first 10 columns of this array contain the raw MOT detection
copied over from the input file. The remaining 128 columns store the appearance
descriptor. The files generated by this command can be used as input for the
`deep_sort_app.py`.
``generate_detections.py``为MOT16数据集的每个序列存储一个单独的二进制文件，格式为NumPy本机格式。
每个文件包含一个形状为Nx138的数组，其中N是相应MOT序列中检测到的数目。
这个数组的前10列包含从输入文件复制过来的原始MOT检测。
其余128列存储外观描述符。这个命令生成的文件可以用作deep_sort_app.py的输入。

**NOTE**: If ``python tools/generate_detections.py`` raises a TensorFlow error,
try passing an absolute path to the ``--model`` argument. This might help in
some cases.

## Training the model

To train the deep association metric model we used a novel [cosine metric learning]
(https://github.com/nwojke/cosine_metric_learning) approach which is provided as a separate repository.

## Highlevel overview of source files

In the top-level directory are executable scripts to execute, evaluate, and
visualize the tracker. The main entry point is in `deep_sort_app.py`.
This file runs the tracker on a MOTChallenge sequence.

In package `deep_sort` is the main tracking code:

* `detection.py`: Detection base class.
* `kalman_filter.py`: A Kalman filter implementation and concrete
   parametrization for image space filtering.
* `linear_assignment.py`: This module contains code for min cost matching and
   the matching cascade.
* `iou_matching.py`: This module contains the IOU matching metric.
* `nn_matching.py`: A module for a nearest neighbor matching metric.
* `track.py`: The track class contains single-target track data such as Kalman
  state, number of hits, misses, hit streak, associated feature vectors, etc.
* `tracker.py`: This is the multi-target tracker class.

The `deep_sort_app.py` expects detections in a custom format, stored in .npy
files. These can be computed from MOTChallenge detections using
`generate_detections.py`. We also provide
[pre-generated detections](https://drive.google.com/open?id=1VVqtL0klSUvLnmBKS89il1EKC3IxUBVK).

## Citing DeepSORT

If you find this repo useful in your research, please consider citing the following papers:

    @inproceedings{Wojke2017simple,
      title={Simple Online and Realtime Tracking with a Deep Association Metric},
      author={Wojke, Nicolai and Bewley, Alex and Paulus, Dietrich},
      booktitle={2017 IEEE International Conference on Image Processing (ICIP)},
      year={2017},
      pages={3645--3649},
      organization={IEEE},
      doi={10.1109/ICIP.2017.8296962}
    }

    @inproceedings{Wojke2018deep,
      title={Deep Cosine Metric Learning for Person Re-identification},
      author={Wojke, Nicolai and Bewley, Alex},
      booktitle={2018 IEEE Winter Conference on Applications of Computer Vision (WACV)},
      year={2018},
      pages={748--756},
      organization={IEEE},
      doi={10.1109/WACV.2018.00087}
    }


MOT16数据集分为了test和train，每个文件夹有7个子文件夹。
以训练集中的MOT16-02为例，探究数据数据集格式：
MOT16-02下包含三个四个子文件：det、gt、img1、seqinfo.ini

det下只有一个文件，det.txt。每行一个标注，代表一个检测物体
[0]:第几帧（可以看到img1中共600帧图）
[1]:目标运动轨迹编号（在目标文件中都为-1）
[2-6]:bbox的坐标尺寸
[6]分类的置信度分数
[7:10]:<x, y, z> 用于3D检测，2D检测总是为1

img1这个目录就是把视频一帧帧抽取出来的图片，总共600张。文件命名从000001.jpg到000600.jpg。

gt文件夹下只有一个文件，gt.txt
第1个值：视频帧号
第2个值：目标运动轨迹的ID号
第3-6值：bbox坐标（x, y, w, h）
第7个值：目标轨迹是否进入考虑范围（0：忽略， 1：active）
第8个值：该轨迹对应的目标种类
第9个值：box的visibility ratio，表示目标运动时被其他目标box包含/覆盖或者目标之间box边缘裁剪情况。

import numpy as np
original=np.load('./resources/detections/MOT16_POI_test/MOT16-01.npy')
print(original.shape)  (10236, 138)   (95758, 138)
original=np.load('./resources/detections/MOT16_POI_test/MOT16-03.npy')
test=np.load('./resources/detections/MOT16_test/MOT16-01.npy')
print(test.shape)      (3775, 138)    (85854, 138)
test=np.load('./resources/detections/MOT16_test/MOT16-03.npy')
test_market=np.load('./resources/detections/MOT16_test_market1501/MOT16-01.npy')
print(test_market.shape)  (3775, 138)
test_mars=np.load('./resources/detections/MOT16_test_mars/MOT16-01.npy')
print(test_mars.shape)    (3775, 138)

